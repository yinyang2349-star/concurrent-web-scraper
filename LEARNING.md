# ğŸš€ Concurrent Web Scraper

A production-ready web scraper built with Go, focusing on clean architecture, proper error handling, and context management.

## ğŸ“š Learning Project

This is Week 1 of my journey to become an expert software engineer in 2026. The goal is to master Go fundamentals, interfaces, error handling, and the context package.

## âœ¨ Features

- âœ… Interface-based design for testability
- âœ… Custom error types with error wrapping
- âœ… Context support for cancellationn and timeouts
- âœ… Multiple URL scraping with result tracking
- âœ… Duration and timestamp tracking
- ğŸ”„ Concurrency support (coming in Week 2)

## ğŸ—ï¸ Architecture

```
concurrent-web-scraper/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ scraper/
â”‚       â””â”€â”€ main.go          # Entry point
â”œâ”€â”€ internal/
â”‚   â””â”€â”€ scraper/
â”‚       â”œâ”€â”€ fetcher.go       # HTTP fetcher implementation
â”‚       â”œâ”€â”€ errors.go        # Custom error types
â”‚       â”œâ”€â”€ result.go        # Result data structure
â”‚       â””â”€â”€ scraper.go       # Scraper orchestrator
â”œâ”€â”€ pkg/                     # (future:  reusable packages)
â”œâ”€â”€ test/                    # (future: tests)
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ README.md
â””â”€â”€ LEARNING.md
```

## ğŸš€ Usage

```bash
# Clone repository
git clone https://github.com/yinyang2349-star/concurrent-web-scraper.git
cd concurrent-web-scraper

# Run scraper
go run cmd/scraper/main.go
```

### Basic Usage

```go
package main

import (
	"context"
	"fmt"
	"time"

	"github.com/yinyang2349-star/concurrent-web-scraper/internal/scraper"
)

func main() {
	fmt.Println("Starting web scraper...")
	fmt.Println("ğŸš€ Day 5 - Multiple URLs")

	// Create scraper instance
	fetcher := scraper.NewHTTPFetcher()
	scr := scraper.NewScraper(fetcher)

	// URLs to scrape
	urls := []string{
		"https://example.com",
		"https://httpstat.us/200",
		"https://httpstat.us/404",
		"https://httpstat.us/500",
		"https://thisurldoesnotexist.tld", // Invalid URL to test error handling
	}

	// Scrape with timeout context
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	fmt.Printf("Scraping %d URLs with 30 seconds timeout...\n\n", len(urls))
	start := time.Now()

	results := scr.Scrape(ctx, urls)

	totalDuration := time.Since(start)

	// Print results
	successCount := 0
	for _, result := range results {
		if result.Success() {
			fmt.Printf("âœ… Fetched %s in %v (Content Length: %d bytes)\n", result.URL, result.Duration, len(result.Content))
			successCount++
		} else {
			fmt.Printf("âŒ Failed to fetch %s in %v (Error: %v)\n", result.URL, result.Duration, result.Error)

		}
	}

	// Summary
	fmt.Println("-------------------------------------")
	fmt.Printf("\n--- Summary ---")
	fmt.Printf("\nTotal URLs: %d", len(urls))
	fmt.Printf("\nSuccessful fetches: %d", successCount)
	fmt.Printf("\nFailed fetches: %d", len(urls)-successCount)
	fmt.Printf("\nTotal duration: %v\n", totalDuration)

}

```

## ğŸ› ï¸ Tech Stack

- **Language:** Go 1.21+
- **Standard Library:** net/http, context, errors, io

## ğŸ“ˆ Roadmap

- [x] Week 1: Fundamentals (interfaces, errors, context)
- [ ] Week 2: Concurrency (goroutines, channels, worker pools)
- [ ] Week 3: Testing (unit tests, table-driven tests, mocks)
- [ ] Week 4: CLI & Features (Cobra, rate limiting, output formats)

## ğŸ“ Learning Resources

- [Effective Go](https://go.dev/doc/effective_go)
- [Go by Example](https://gobyexample.com)
- [Go Blog - Error Handling](https://go.dev/blog/error-handling-and-go)
- [Go Blog - Context](https://go.dev/blog/context)

## ğŸ“ Development Log

See [LEARNING.md](LEARNING.md) for daily learning notes and progress.

## ğŸ‘¤ Author

**yinyang2349-star**

- GitHub: [@yinyang2349-star](https://github.com/yinyang2349-star)
- Learning Journey: [2026 Expert Software Engineer Roadmap]

## ğŸ“„ License

MIT License - feel free to use for learning!

---

**â­ Star this repo if you're also on a learning journey!**
